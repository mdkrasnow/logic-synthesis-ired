---
description: Solve a problem using multi-agent debate (MAD) to find the optimal solution to a problem through 3 rounds of parallel deliberation
argument-hint: <problem-description>
allowed-tools: Write, Read, SubAgent, Bash, Bash(cat:*), Touch, Echo
model: claude-sonnet-4-5
---

# Multi-Agent Debugging Debate Solver

You are the orchestrator for a Multi-Agent Debate (MAD) system. Your job is to coordinate 3 solution agents through 3 rounds of debate to find the optimal solution to a problem.

In this debugging use case, the "problem" is:
- to identify and explain the bugs that are breaking training,
- to propose concrete fixes and validation steps.

All agents must:
- Assume there is at least one real bug in the codebase.
- Treat their work as **debugging** (finding bugs, failure modes, inconsistencies, and fixes).
- **Not** assume any specific *type* or *structure* of bug ahead of time.
- Use their assigned perspective only as a **thinking style** (e.g., simplicity, robustness, maintainability), not as a pre-commitment to a particular bug category.

## Input
Problem to solve: "$ARGUMENTS"

## Process Overview

You will coordinate a **fixed 3-round debate** with **parallel agent execution**:
- **Round 1**: Independent generation (3 agents, Haiku, parallel)
- **Round 2**: Adversarial critique (3 agents, Sonnet, parallel)
- **Round 3**: Cooperative synthesis (3 agents, Sonnet, parallel)
- **Final**: Single synthesis agent produces the final solution (Sonnet)

## Execution Steps

### Step 1: Setup Workspace

Create the debate workspace:

```bash
mkdir -p .claude/work/debate/round-1
mkdir -p .claude/work/debate/round-2
mkdir -p .claude/work/debate/round-3
````

Write the problem statement to `.claude/work/debate/problem.md`:

```markdown
# Problem Statement

{user's problem description from $ARGUMENTS}

---
Generated: {timestamp}
Debate ID: {generate random ID}
```

### Step 2: Round 1 - Independent Generation (Parallel, Haiku)

In Round 1, each agent independently:

* Reads the problem statement.
* Assumes there is at least one real bug affecting training.
* Searches for bugs, inconsistencies, and plausible failure modes.
* Proposes explanations and potential fixes.
* Uses its assigned **perspective as a reasoning lens**, without assuming any specific bug structure.

Invoke the `solution-debater` sub-agent **3 times in parallel** with different configurations:

**Agent 1 Configuration:**

* agent_id: "agent-1"
* perspective: "simplicity"

  * This agent favors simple, direct explanations of the observed failure (e.g., straightforward implementation mistakes, configuration issues, or logic errors), but it must not assume any particular bug type in advance.
* round: 1
* model: haiku
* critique_mode: "none"
* output_path: ".claude/work/debate/round-1/agent-1.json"

**Agent 2 Configuration:**

* agent_id: "agent-2"
* perspective: "robustness"

  * This agent focuses on whether the system is fragile or brittle under reasonable conditions (e.g., edge cases, implicit assumptions, missing checks), but it must not pre-assume what the bug is.
* round: 1
* model: haiku
* critique_mode: "none"
* output_path: ".claude/work/debate/round-1/agent-2.json"

**Agent 3 Configuration:**

* agent_id: "agent-3"
* perspective: "maintainability"

  * This agent examines how the structure, clarity, and interfaces of the implementation might hide or induce bugs, again without assuming any specific bug category ahead of time.
* round: 1
* model: haiku
* critique_mode: "none"
* output_path: ".claude/work/debate/round-1/agent-3.json"

**How to invoke in parallel:**

For each agent, create a prompt that includes:

1. The problem statement from `.claude/work/debate/problem.md`
2. The agent configuration (id, perspective, round, critique_mode)
3. A reminder that:

   * There is at least one real bug.
   * The agentâ€™s job is to **find bugs and propose fixes**.
   * The agent must **not** assume any specific bug type in advance; it should let the evidence and reasoning guide the hypotheses.
4. Instruction to write output to the specified path in a structured format (e.g., JSON with sections for hypotheses, evidence, proposed fixes, and open questions).

Invoke all 3 agents and **wait for all to complete** before proceeding to Round 2.

**After Round 1 completes:**

* Read all 3 output files.
* Create `.claude/work/debate/round-1/summary.md` with a brief overview of the 3 proposed debugging analyses, including:

  * The main bug hypotheses each agent proposes.
  * Key evidence they rely on.
  * Any overlapping or conflicting explanations.
  * Lists of proposed code changes or checks.

### Step 3: Round 2 - Adversarial Critique (Parallel, Sonnet)

In Round 2, each agent:

* Reads its own Round 1 output and the other agentsâ€™ outputs.
* Enters **adversarial critique mode**, focusing on:

  * Challenging weak or unsupported bug hypotheses.
  * Pointing out logical gaps or missing evidence.
  * Proposing tests or diagnostics that would distinguish between competing explanations.
* Still does **not** assume any specific bug type, but treats all hypotheses as candidates to be tested and attacked.

Invoke the `solution-debater` sub-agent **3 times in parallel** with adversarial critique mode:

**Agent 1 Configuration:**

* agent_id: "agent-1"
* perspective: "simplicity"
* round: 2
* model: sonnet
* critique_mode: "adversarial"
* own_previous: ".claude/work/debate/round-1/agent-1.json"
* others_previous: [".claude/work/debate/round-1/agent-2.json", ".claude/work/debate/round-1/agent-3.json"]
* output_path: ".claude/work/debate/round-2/agent-1.json"

**Agent 2 Configuration:**

* agent_id: "agent-2"
* perspective: "robustness"
* round: 2
* model: sonnet
* critique_mode: "adversarial"
* own_previous: ".claude/work/debate/round-1/agent-2.json"
* others_previous: [".claude/work/debate/round-1/agent-1.json", ".claude/work/debate/round-1/agent-3.json"]
* output_path: ".claude/work/debate/round-2/agent-2.json"

**Agent 3 Configuration:**

* agent_id: "agent-3"
* perspective: "maintainability"
* round: 2
* model: sonnet
* critique_mode: "adversarial"
* own_previous: ".claude/work/debate/round-1/agent-3.json"
* others_previous: [".claude/work/debate/round-1/agent-1.json", ".claude/work/debate/round-1/agent-2.json"]
* output_path: ".claude/work/debate/round-2/agent-3.json"

Invoke all 3 agents and **wait for all to complete** before proceeding to Round 3.

**After Round 2 completes:**

* Read all 3 output files.
* Create `.claude/work/debate/round-2/summary.md` noting:

  * Which bug hypotheses were strongly challenged or weakened.
  * Which bug hypotheses survived critique and appear more plausible.
  * New diagnostics, tests, or code inspections suggested by the agents.
  * Areas of growing consensus vs persistent disagreement.

### Step 4: Round 3 - Cooperative Synthesis (Parallel, Sonnet)

In Round 3, each agent:

* Reads its own Round 2 output and the other agentsâ€™ Round 2 outputs.
* Enters **cooperative mode**, focusing on:

  * Building a shared, coherent picture of what the bug(s) most likely are.
  * Combining partial insights into stronger, unified hypotheses.
  * Proposing a concrete, ordered plan to investigate, fix, and validate the suspected bugs.

Invoke the `solution-debater` sub-agent **3 times in parallel** with cooperative mode:

**Agent 1 Configuration:**

* agent_id: "agent-1"
* perspective: "simplicity"
* round: 3
* model: sonnet
* critique_mode: "cooperative"
* own_previous: ".claude/work/debate/round-2/agent-1.json"
* others_previous: [".claude/work/debate/round-2/agent-2.json", ".claude/work/debate/round-2/agent-3.json"]
* output_path: ".claude/work/debate/round-3/agent-1.json"

**Agent 2 Configuration:**

* agent_id: "agent-2"
* perspective: "robustness"
* round: 3
* model: sonnet
* critique_mode: "cooperative"
* own_previous: ".claude/work/debate/round-2/agent-2.json"
* others_previous: [".claude/work/debate/round-2/agent-1.json", ".claude/work/debate/round-2/agent-3.json"]
* output_path: ".claude/work/debate/round-3/agent-2.json"

**Agent 3 Configuration:**

* agent_id: "agent-3"
* perspective: "maintainability"
* round: 3
* model: sonnet
* critique_mode: "cooperative"
* own_previous: ".claude/work/debate/round-2/agent-3.json"
* others_previous: [".claude/work/debate/round-2/agent-1.json", ".claude/work/debate/round-2/agent-2.json"]
* output_path: ".claude/work/debate/round-3/agent-3.json"

Invoke all 3 agents and **wait for all to complete** before proceeding to synthesis.

**After Round 3 completes:**

* Read all 3 output files.
* Create `.claude/work/debate/round-3/summary.md` noting:

  * The final converged set of bug hypotheses.
  * The proposed investigation and fix plan (ordered steps).
  * Any remaining uncertainties or alternative explanations that should be kept in mind.

### Step 5: Final Synthesis

Invoke the `synthesis-agent` sub-agent **once** with the complete debate history:

**Synthesis Configuration:**

* problem: ".claude/work/debate/problem.md"
* round_1_outputs: all 3 agent outputs from round-1/
* round_2_outputs: all 3 agent outputs from round-2/
* round_3_outputs: all 3 agent outputs from round-3/
* output_path: ".claude/work/debate/final-synthesis.md"

The synthesis agent will:

* Review the full debugging debate.
* Identify the most plausible bug explanations and their interactions.
* Produce a final, ordered plan of:

  * What to inspect.
  * What to change in the code.
  * How to test and validate that the bug(s) are fixed.
* Clearly separate:

  * High-confidence conclusions.
  * Medium-confidence hypotheses.
  * Low-confidence/remaining open questions.

### Step 6: Present Results

After synthesis completes:

1. **Read** `.claude/work/debate/final-synthesis.md`

2. **Present to user:**

   ```markdown
   # ðŸŽ¯ Multi-Agent Debate Results

   After 3 rounds of debate with 3 solution agents, here is the optimal debugging solution:

   {paste the Executive Summary section from final-synthesis.md}

   ðŸ“„ **Full Debugging Plan**: [View final-synthesis.md](computer:///.claude/work/debate/final-synthesis.md)

   ## Debate Process
   - âœ… Round 1: Independent generation (3 diverse debugging perspectives)
   - âœ… Round 2: Adversarial critique (identified weaknesses in bug hypotheses)
   - âœ… Round 3: Cooperative synthesis (refined consensus on bugs and fixes)

   ## Debate Transcripts
   You can review the full debate history:
   - [Round 1 Summary](computer:///.claude/work/debate/round-1/summary.md)
   - [Round 2 Summary](computer:///.claude/work/debate/round-2/summary.md)
   - [Round 3 Summary](computer:///.claude/work/debate/round-3/summary.md)

   Would you like me to:
   1. Show the detailed proposed code changes?
   2. Explain any specific debugging decision?
   3. Propose regression tests to prevent this bug in the future?
   ```

## Implementation Notes

**Parallel Execution:**
When invoking agents "in parallel," you should:

1. Prepare prompts for all 3 agents.
2. Invoke each agent via the SubAgent tool.
3. Do NOT wait for one to finish before starting the next.
4. After all 3 are invoked, gather results from their output files.

**Model Selection:**

* Round 1: Use `model: claude-3-5-haiku` (fast, diverse).
* Round 2-3: Use `model: claude-sonnet-4-5` (deep reasoning).
* Synthesis: Use `model: claude-sonnet-4-5` (high quality).

**Error Handling:**
If any agent fails to write output:

* Note the failure in the summary.
* Continue with available outputs.
* Flag this in the final synthesis as a limitation or confidence reduction.

## Exit Criteria

You are done when:

* All 3 rounds complete.
* Synthesis produces `.claude/work/debate/final-synthesis.md`.
* User is presented with the final debugging solution.

This is **planning only**. The output is a detailed debugging and solution plan, NOT implemented code.